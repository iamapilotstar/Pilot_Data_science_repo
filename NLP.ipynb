{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import math\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# the code block below is directly downloading commentary.txt and superheros.csv into your drive folder. Please just run it and do not comment out.\n",
        "from urllib import request\n",
        "module_url = [f\"https://drive.google.com/uc?export=view&id=18y6hLv2bqAyJsIXwVCty58lF0u7yimVq\"]\n",
        "name = ['commentary.txt']\n",
        "for i in range(len(name)):\n",
        "    with request.urlopen(module_url[i]) as f, open(name[i],'w') as outf:\n",
        "        a = f.read()\n",
        "        outf.write(a.decode('ISO-8859-1'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G9HTRZ9HVkO",
        "outputId": "868a6a11-fbe2-4213-a9a2-549d2071268e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNwbpUe0HGXx"
      },
      "outputs": [],
      "source": [
        "def get_word(treebank_tag):\n",
        "    \"\"\"\n",
        "    The function takes Penn Treebank part-of-speech tags and converts them to wordnet tags.\n",
        "\n",
        "    Parameters used:\n",
        "        treebank_tag (str): The Penn Treebank part-of-speech tag.\n",
        "\n",
        "    Returns:\n",
        "        str: It returns the corresponding WordNet tag, or an empty string if the tag is not recognized.\n",
        "    \"\"\"\n",
        "    # Converting the treebank tags to wordnet tags\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return nltk.corpus.wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return nltk.corpus.wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return nltk.corpus.wordnet.ADV\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "   The function tokenizes text once it has been converted to lowercase.\n",
        "\n",
        "    Parameters used:\n",
        "        text (str): The text to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        list: It returns a list of tokens (words) from the given text.\n",
        "    \"\"\"\n",
        "    # Converting the text to lower case and tokenize\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "def pos_tagging(tokens):\n",
        "    \"\"\"\n",
        "    The function perform part-of-speech tagging on a list of tokens.\n",
        "\n",
        "    Parameters used:\n",
        "        tokens (list): A list of tokens (words) to tag.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, each containing a token and associated part-of-speech tag.\n",
        "    \"\"\"\n",
        "    # Apply POS tagging\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "def lemmatize(pos_tags):\n",
        "    \"\"\"\n",
        "    The function lemmatize a list of tokens based on their part-of-speech tags.\n",
        "\n",
        "   Parameters used:\n",
        "        pos_tags (list): A collection of tuples, each containing a token and associated part-of-speech tag.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, each containing a lemmatized token and the part-of-speech tag.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = []\n",
        "    # Lemmatizing words based on their POS tags\n",
        "    for lem_words, tags  in pos_tags:\n",
        "        pos = get_word(tags)\n",
        "        lemma = lemmatizer.lemmatize(lem_words, pos) if pos else lemmatizer.lemmatize(lem_words)\n",
        "        lemmatized.append((lemma, tags))\n",
        "    return lemmatized\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('commentary.txt', delimiter='\\t')\n",
        "\n",
        "# Apply the preprocessing functions\n",
        "df['Tokenized'] = df['Commentary'].apply(tokenize)\n",
        "df['PoS_tagged'] = df['Tokenized'].apply(pos_tagging)\n",
        "df['PoS_lemmatized'] = df['PoS_tagged'].apply(lemmatize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word(treebank_tag):\n",
        "        \"\"\"\n",
        "        The function converts Penn Treebank tags to a format compatible with the WordNet Lemmatize.\n",
        "\n",
        "        Parameters used:\n",
        "                treebank_tag (str): A part-of-speech tag in Penn Treebank format.\n",
        "        Returns:\n",
        "           A compatible WordNet part-of-speech tag.\n",
        "\n",
        "        \"\"\"\n",
        "        if treebank_tag.startswith('J'):\n",
        "                return nltk.corpus.wordnet.ADJ\n",
        "\n",
        "        elif treebank_tag.startswith('V'):\n",
        "               return nltk.corpus.wordnet.VERB\n",
        "\n",
        "        elif treebank_tag.startswith('N'):\n",
        "                return nltk.corpus.wordnet.NOUN\n",
        "\n",
        "        elif treebank_tag.startswith('R'):\n",
        "                return nltk.corpus.wordnet.ADV\n",
        "\n",
        "        else:\n",
        "                return ''  # Return an empty string for unknown tags\n",
        "\n",
        "def lemmatize(query):\n",
        "\n",
        "    \"\"\"\n",
        "    The function Lemmatizes a text query by converting it to lowercase, tokenizing it and applying part-of-speech tagging. It also+ lemmatizing each word based on its POS tag.\n",
        "\n",
        "    Parameters used:\n",
        "            query (str): The text query should be lemmatized.\n",
        "\n",
        "    Returns:\n",
        "        A set of unique lemmatized words from the query.\n",
        "\n",
        "    \"\"\"\n",
        "    # Tokenizing the query after converting it to lowercase\n",
        "    tkn = word_tokenize(query.lower())\n",
        "\n",
        "    # Apply part-of-speech tagging to the tokens\n",
        "    pos_t = nltk.pos_tag(tkn)\n",
        "\n",
        "    # Initialize the WordNet Lemmatizer\n",
        "    lemmatize = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize each word based on its part-of-speech tag\n",
        "    lemmatized_words= [lemmatize.lemmatize(word, get_word(pos) or nltk.corpus.wordnet.NOUN) for word, pos in pos_t]\n",
        "\n",
        "    # Return a set of unique lemmatized words\n",
        "    return set(lemmatized_words)\n",
        "\n",
        "def retrieve_similar_commentaries(df, query, k):\n",
        "\n",
        "        \"\"\"\n",
        "        The function obtains the top 'k' comparable commentaries from a DataFrame based on lemmatized token similarity between the commentaries and the query, To highlight the relevance of matched words, a bonus is added to their similarity score.\n",
        "\n",
        "        Parameters used:\n",
        "                  df (DataFrame): The Dataframe consisting of commentaries with a column 'PoS_lemmatized'.\n",
        "                  query (str): The text query to find similar commentaries.\n",
        "                  k (int): Number of top similar commentaries to retrieve.\n",
        "\n",
        "        Returns:\n",
        "             A list of tuples from where each tuple contains the commentary text and its similarity score.\n",
        "\n",
        "        \"\"\"\n",
        "        # Tokenize and lemmatize the provided query using a predefined function\n",
        "        querytkn = lemmatize(query)\n",
        "\n",
        "        # Define a function to calculate the similarity score between the query tokens and commentary tokens\n",
        "        def calculate_similarity(tokens):\n",
        "                # Calculate tokens between the query and the commentary\n",
        "                com_tokens = querytkn.intersection(set(token for token, _ in tokens))\n",
        "\n",
        "                # Calculate a bonus for common nouns to emphasize their importance in similarity\n",
        "                noun = sum(1 for token in com_tokens if any(t[1].startswith('N') for t in tokens if t[0] == token))\n",
        "\n",
        "                # The final similarity score is the count of common tokens plus the noun bonus\n",
        "                return len(com_tokens) + noun\n",
        "\n",
        "        # Apply the similarity calculation to each commentary in the DataFrame\n",
        "        df['similarity'] = df['PoS_lemmatized'].apply(calculate_similarity)\n",
        "\n",
        "        # Retrieve the top 'k' commentaries based on their calculated similarity scores\n",
        "        top_k = df.nlargest(k, 'similarity')[['Commentary', 'similarity']]\n",
        "\n",
        "        # Return the top commentaries as a list of tuples without the DataFrame index\n",
        "        return list(top_k.itertuples(index=False, name=None))"
      ],
      "metadata": {
        "id": "Jk2jhL2FJZSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_phrases(token_tuples):\n",
        "    \"\"\"Extract noun phrases (NPs) and verb phrases (VPs) based on POS tags.\n",
        "\n",
        "    Parameters:\n",
        "    token_tuples (list): A list of tuples containing word and its POS tag.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two lists, the first list contains noun phrases (NPs)\n",
        "    and the second list contains verb phrases (VPs).\n",
        "    \"\"\"\n",
        "    noun_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS'}\n",
        "    verb_pos_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
        "    noun_phrases, verb_phrases = [], []\n",
        "    current_noun, current_verb = [], []\n",
        "\n",
        "    for word, pos_tag in token_tuples:\n",
        "        if pos_tag in noun_pos_tags:\n",
        "            if current_verb:\n",
        "                verb_phrases.append(' '.join(current_verb))\n",
        "                current_verb = []\n",
        "            current_noun.append(word)\n",
        "        elif pos_tag in verb_pos_tags:\n",
        "            if current_noun:\n",
        "                noun_phrases.append(' '.join(current_noun))\n",
        "                current_noun = []\n",
        "            current_verb.append(word)\n",
        "        else:\n",
        "            if current_noun:\n",
        "                noun_phrases.append(' '.join(current_noun))\n",
        "                current_noun = []\n",
        "            if current_verb:\n",
        "                verb_phrases.append(' '.join(current_verb))\n",
        "                current_verb = []\n",
        "\n",
        "    if current_noun:\n",
        "        noun_phrases.append(' '.join(current_noun))\n",
        "    if current_verb:\n",
        "        verb_phrases.append(' '.join(current_verb))\n",
        "\n",
        "    return noun_phrases, verb_phrases\n",
        "\n",
        "def count_phrases(dataframe):\n",
        "    \"\"\"Count the frequency of NPs and VPs.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (DataFrame): A DataFrame where 'PoS_lemmatized' column contains lists of tuples (word, POS).\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two dictionaries, the first dictionary contains counts of NPs\n",
        "    and the second dictionary contains counts of VPs.\n",
        "    \"\"\"\n",
        "    noun_counts = {}\n",
        "    verb_counts = {}\n",
        "\n",
        "    for _, row in dataframe.iterrows():\n",
        "        nps, vps = extract_phrases(row['PoS_lemmatized'])\n",
        "        for np in nps:\n",
        "            noun_counts[np] = noun_counts.get(np, 0) + 1\n",
        "        for vp in vps:\n",
        "            verb_counts[vp] = verb_counts.get(vp, 0) + 1\n",
        "\n",
        "    return noun_counts, verb_counts\n",
        "\n",
        "def compute_pmi_dataframe(dataframe):\n",
        "    \"\"\"Compute the PMI matrix.\n",
        "\n",
        "    Compute the Pointwise Mutual Information (PMI) matrix based on the noun phrases (NPs)\n",
        "    and verb phrases (VPs) extracted from the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (DataFrame): A DataFrame where 'PoS_lemmatized' column contains lists of tuples (word, POS).\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame representing the PMI matrix with noun phrases (NPs) as rows and verb phrases (VPs) as columns.\n",
        "    \"\"\"\n",
        "    # Extract and count phrases\n",
        "    noun_counts, verb_counts = count_phrases(dataframe)\n",
        "\n",
        "    # Calculate total counts\n",
        "    total_nouns = sum(noun_counts.values())\n",
        "    total_verbs = sum(verb_counts.values())\n",
        "\n",
        "    # Find the top 100 NPs and VPs\n",
        "    top_nouns = sorted(noun_counts, key=noun_counts.get, reverse=True)[:100]\n",
        "    top_verbs = sorted(verb_counts, key=verb_counts.get, reverse=True)[:100]\n",
        "\n",
        "    # Create PMI matrix\n",
        "    pmi_matrix = pd.DataFrame(index=top_verbs, columns=top_nouns, data=0.0)\n",
        "\n",
        "    for np in top_nouns:\n",
        "        for vp in top_verbs:\n",
        "            p_joint = (noun_counts.get(np, 0) * verb_counts.get(vp, 0)) / (total_nouns * total_verbs) if noun_counts.get(np, 0) and verb_counts.get(vp, 0) else 0\n",
        "            if p_joint > 0:\n",
        "                pmi_matrix.at[vp, np] = p_joint / ((noun_counts.get(np, 0) / total_nouns) * (verb_counts.get(vp, 0) / total_verbs))\n",
        "\n",
        "    return pmi_matrix\n",
        "\n",
        "data = {\n",
        "    'PoS_lemmatized': [\n",
        "        [('cat', 'NN'), ('run', 'VB'), ('fast', 'RB')],\n",
        "        [('dog', 'NN'), ('jump', 'VB'), ('high', 'JJ')],\n",
        "        [('bird', 'NN'), ('sing', 'VB'), ('beautifully', 'RB')],\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute PMI matrix\n",
        "pmi_df = compute_pmi_dataframe(df)"
      ],
      "metadata": {
        "id": "le21-5d5JcE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}