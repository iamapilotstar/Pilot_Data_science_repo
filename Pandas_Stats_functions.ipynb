{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import accuracy_score, RocCurveDisplay, auc, roc_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.regression.linear_model as sm\n",
        "from sklearn.linear_model import LassoCV\n",
        "# the code block below is directly downloading data into your drive folder. Please just run it and do not comment out.\n",
        "from urllib import request\n",
        "module_url = [f\"https://drive.google.com/uc?export=view&id=1NJN0VmS4lWfrGQ86-glTXU_WQeFX0xHJ\",\n",
        "              f\"https://drive.google.com/uc?export=view&id=1DGhCMqrxRy_oRZDR7cy2OFDqjI_OHydR\"]\n",
        "name = ['STAT.csv', 'nba.csv']\n",
        "for i in range(len(name)):\n",
        "    with request.urlopen(module_url[i]) as f, open(name[i],'w') as outf:\n",
        "        a = f.read()\n",
        "        outf.write(a.decode('ISO-8859-1'))"
      ],
      "metadata": {
        "id": "f9-xw1NI9BFC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sport_viz(df, *args):\n",
        "    \"\"\"\n",
        "    Intro:\n",
        "     Build a function to generate 3 different plots/graphs using Pandas, Seaborn, and Plotly.\n",
        "\n",
        "    Functionality:\n",
        "     This function allows you to generate 3 plots such as bar, box, radar for exploring and analyzing football players statistics.\n",
        "     It offers flexibility in selecting the columns to visualize, aggregate the columns and also add addi parameters like plot size, color scheme, and data filtering option.\n",
        "\n",
        "    Parameters used:\n",
        "        df (pd.DataFrame): The dataframe consisting of football player's statistics.\n",
        "        *args (tuple): It specifies the arguments used for the function.\n",
        "             sel (str): Selects different plots such as - bar, box, and radar.\n",
        "             stats (list of str): The returned list of statistics which will be plotted and visualized.\n",
        "             f (func): The aggregate function for performing grouping operations.\n",
        "             col (str): Selects the necessary target columns.\n",
        "             addi (dict): Extra/additional parameters used in different operations addi = {'n': 2, 'figsize':[4,4], 'colormap': 'Blues', 'notch': True, 'spe_rows': []}.\n",
        "\n",
        "             bar - It creates a barplot and applies grouping with .nlargest function.\n",
        "             box - It creates a boxplot and filters data using spe_row.\n",
        "             radar -It creates a radar plot filters data using spe_row and performs grouping.\n",
        "\n",
        "            fig.update_layout - It adds adjustments such as title, xlabel, ylabel and legends to the radar-plot using plotly\n",
        "            fig.update_traces - It fills the area between the lines and the origin in the radar plot\n",
        "\n",
        "    Returns:\n",
        "      Depending on the test case configuration, this function will generate three distinct types of plots: Bar, Box, and Radar, Each plot type will display the respective statistics as specified in the test case.\n",
        "\n",
        "    \"\"\"\n",
        "    # Error handling for function arguments\n",
        "    if len(args) != 5:\n",
        "        return 'Sorry, incorrect number of arguments provided. Please check and try again!'\n",
        "\n",
        "    # Defining the arguments\n",
        "    sel, col, stats, f, addi = args\n",
        "\n",
        "    # Defining default settings based on question's requirement\n",
        "    n = addi.get('n', 2)\n",
        "    figsize = addi.get('figsize', [4, 4])\n",
        "    colormap = addi.get('colormap', 'Blues')\n",
        "    notch = addi.get('notch', True)\n",
        "    spe_rows = addi.get('spe_rows', [])\n",
        "\n",
        "    # Creates a bar plot and performs groupuing with nth largest.\n",
        "    if sel == 'bar':\n",
        "        if spe_rows:\n",
        "            df = df[df[col].isin(spe_rows)]\n",
        "\n",
        "        grp_bar = df.groupby(col)[stats].agg(f).reset_index()\n",
        "        n_th_largest = grp_bar.nlargest(addi['n'], stats[0], keep='all')\n",
        "        ax = n_th_largest.plot(kind='bar', x=col, figsize=addi['figsize'], colormap=addi['colormap'])\n",
        "\n",
        "        # Adding the figure title, xlabel, ylabel, legend and rotation.\n",
        "        plt.title(f'Bar graph for {col}')\n",
        "        plt.ylabel('Player Values')\n",
        "        plt.xlabel(col)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title=\"Statistics\")\n",
        "        plt.show() # Used to present the visualization in the notebook file\n",
        "        return ax\n",
        "\n",
        "    # Creates a box plot and perfoms filtering on spe_rows\n",
        "    elif sel == 'box':\n",
        "        # Filter DataFrame based on spe_rows\n",
        "        filtered_df = df[df[col].isin(addi['spe_rows'])] if addi['spe_rows'] else df\n",
        "\n",
        "        # Check if any data is available after filtering\n",
        "        if not filtered_df.empty:\n",
        "            plt.figure(figsize=addi['figsize'])  # Set figure size\n",
        "            sns.boxplot(data=filtered_df[stats], palette=addi['colormap'], notch=addi['notch'])\n",
        "            plt.title(f'Box Plot') # Set the title of the box plot\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('Statistics')\n",
        "            plt.xticks(rotation=45) # Set the rotation of xticks to replicate similar output as in test case\n",
        "            plt.show() # Used to present the visualization in the notebook file\n",
        "        else:\n",
        "            print('Apologies, no available data found after filtering')\n",
        "\n",
        "    # Creates a radar plot and performs groupuing with nth largest.\n",
        "    elif sel == 'radar':\n",
        "        # Filter DataFrame based on spe_rows if provided\n",
        "        if addi['spe_rows']:\n",
        "            df = df[df[col].isin(addi['spe_rows'])]\n",
        "\n",
        "        if not df.empty:\n",
        "            # limiting the number of players displayed to avoid overcrowding.\n",
        "            grp_radar_df = df.groupby(col)[stats].agg(f).reset_index()\n",
        "            # Ensuring consistent length of data for the radar chart\n",
        "            df_results = grp_radar_df.melt(id_vars=[col], value_vars=stats, var_name='Stat', value_name='Value') # melt the df_results\n",
        "            top_n = df_results[df_results[col].isin(grp_radar_df.nlargest(addi['n'], stats[0])[col])]\n",
        "            fig = px.line_polar(top_n, r='Value', theta='Stat', color=col, line_close=True)\n",
        "            fig.update_traces(fill='toself')\n",
        "            fig.update_layout(width=addi['figsize'][0] * 100, height=addi['figsize'][1] * 100)\n",
        "            fig.show() # Used to present the visualization in the notebook file\n",
        "        else:\n",
        "            print(\"Error No data is available for the selected players, Please try again with other criteria's.\") # Error handling if no fulfilled criteria in test case"
      ],
      "metadata": {
        "id": "iBMrgVhqNrvR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grammy_scraper(url, start, end, PerfQuery):\n",
        "    \"\"\"\n",
        "    This function performs web scraping of data from the Wikipedia page dedicated to the Grammy Award for Best Rock Song-(https://en.wikipedia.org/wiki/Grammy_Award_for_Best_Rock_Song),\n",
        "    it retrieves information about Grammy Award wins for specified performers over various years.\n",
        "\n",
        "    Parameters used:\n",
        "        url (str): The URL of the Wikipedia page containing the Grammy Award data to be scraped.\n",
        "        start (int): The starting year for the search window looking for Grammy data that starts from this year.\n",
        "        end (int): The ending year for the search window looking for Grammy data that end by this year.\n",
        "        PerfQuery (str): The performer's name to search for in the Grammy data table, since the function is case-insensitive it will search for the performer in any case format.\n",
        "\n",
        "   Functionality:\n",
        "   -response - we Send a GET request to specified URL\n",
        "   -soup - we use 'soup' parse the HTML content of the page using BeautifulSoup library\n",
        "   -cols - we define the columns from which the dataframe will be created\n",
        "   -row - Select all table rows in the Grammy Awards table\n",
        "   -year - Extract the year from the row's header cell.\n",
        "   -winner - Find the winner and their work\n",
        "   -data.append - Append the winner's information to the data list\n",
        "   -df_performers - Filter DataFrame for the specified performer and count nominations and wins\n",
        "   -Print a summary of the performer's nominations and wins within the specified date range.\n",
        "\n",
        "\n",
        "\n",
        "    Returns:\n",
        "         (DataFrame): It returns a DataFrame containing all the filtered Grammy data of nominations and wins for the specified performer over the years, This DataFrame will include columns such as [ 'Year', 'Performer', 'Work', and 'Winner'].\n",
        "          Print - It also returns a print statment below the dataframe for example: Between years 2001 and 2012, U2 has been nominated for the Grammy Awards 5 times. Among those nominations, U2 won the award 2 times.\n",
        "  Notes:\n",
        "\n",
        "    BeautifulSoup Library This function utilizes the BeautifulSoup library to scrape data from Wikipedia about Grammy Award-winning performers, it extracts relevant details and returns a filtered DataFrame based on specified criteria.\n",
        "    \"\"\"\n",
        "    # Send a GET request to the specified URL\n",
        "    response = requests.get(url)\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Define the columns of the DataFrame to be created\n",
        "    cols = ['Year', 'Performer', 'Work', 'Winner']\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # Select all table rows in the Grammy Awards table, skipping the header row\n",
        "    rows_in_table = soup.select('div.mw-content-ltr.mw-parser-output table.wikitable tbody tr')[1:]\n",
        "\n",
        "    for row in rows_in_table:\n",
        "        # Extract the year from the row's header cell\n",
        "        year_in_table = row.select_one('th')\n",
        "        if year_in_table:\n",
        "            text = year_in_table.get_text().strip()\n",
        "            if text.isdigit():\n",
        "                year = int(text)\n",
        "                if start <= year <= end:\n",
        "                    # Process only rows within the specified date range\n",
        "                    find_winners = row.select_one('td:nth-child(4)')\n",
        "                    winner_in_table = row.select_one('td:nth-child(3)')\n",
        "                    if find_winners and winner_in_table:\n",
        "                        winner = find_winners.get_text().strip()\n",
        "                        winnerwork = winner_in_table.get_text().strip()\n",
        "                        data.append([year, winner, winnerwork, True])\n",
        "\n",
        "                        # Find all nominees within the list in the 5th column\n",
        "                        list_of_nominees = row.select_one('td:nth-child(5) div ul')\n",
        "                        if list_of_nominees:\n",
        "                            for nominee in list_of_nominees.find_all('li'):\n",
        "                                nominee_word = nominee.get_text()\n",
        "                                if '–' in nominee_word:\n",
        "                                    nominee_parts = nominee_word.split(\"–\", 1)[1].strip()\n",
        "                                    if nominee_parts.count('\" (') == 1:\n",
        "                                        nomineework, nominee = nominee_parts.split('\" (')\n",
        "                                        data.append([year, nominee.strip(),nomineework.strip(), False])\n",
        "\n",
        "    # Create a DataFrame with the collected data\n",
        "    df = pd.DataFrame(data, columns=cols)\n",
        "    print(df.head(),'\\n')\n",
        "\n",
        "    # Filter DataFrame for the specified performer and count nominations and wins\n",
        "    df_performers = df[df['Performer'].str.contains(PerfQuery, case=False, na=False)]\n",
        "    nominated = len(df_performers)\n",
        "    num_times_won = len(df_performers[df_performers['Winner'] == True])\n",
        "\n",
        "    # Print summary information\n",
        "    print(f\"Between years {start} and {end}, {PerfQuery} has been nominated for the Grammy Awards {nominated} times. Among those nominations, {PerfQuery} won the award {num_times_won} times.\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sFatY5WLOxYJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Fextractor(*args):\n",
        "    \"\"\"\n",
        "    This function performs feature selection for the most relevant/important features from in your dataset,\n",
        "    It simplifies the process of choosing between different feature selection techniques such as: variance thresholding, importance, Pearson correlation, and LASSO regularization allowing a tailored approach based on the dataset's characteristics\n",
        "and the specific modeling objectives.\n",
        "\n",
        "    Parameters used:\n",
        "        *args (tuple): It specifies the arguments used for the function.\n",
        "        df (dataset): It consists of the data from which different selection techniques will be performed (nba.csv)\n",
        "        target (str): The Independent variable/ outcome variable\n",
        "        sel (str): It is used to selects different feature selection techniques such as ['var', 'imp', 'cor', 'lasso']:\n",
        "                    'var': It cuts off features with low variance below a specified threshold.\n",
        "                    'imp': Finds the features that are most relevant and important.\n",
        "                    'cor': Finds which features are highly correlated with your target variable.\n",
        "                    'lasso': It effectively reduces the number of features by constraining the size of the coefficients, which helps to enhance model interpretability.\n",
        "        threshold' - List of thresholds for 'var', 'imp', 'cor'.\n",
        "       'imp_func' - The model/function class to use for computing the feature importance in 'imp'.\n",
        "        'cv = 5' -  The number of folds in cross-validation.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        This function returns a list of columns which consists of the most important features which are meticulously filtered based on the methods specified in the test case.\n",
        "    \"\"\"\n",
        "    if len(args) != 4:\n",
        "        return 'Sorry, incorrect number of arguments provided. Kindly check the required number of arguments and try again!'\n",
        "\n",
        "    df, target, sel, addi = args\n",
        "\n",
        "    # Drop rows with any NaN values in the dataframe\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Drop non-numeric columns for feature selection methods that need numeric input\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if target in num_cols:\n",
        "        num_cols.remove(target)\n",
        "\n",
        "    if target not in df.columns:\n",
        "        return 'The target variable specified is not in the dataset.'\n",
        "\n",
        "    X = df[num_cols] # Independent variable\n",
        "    y = df[target] # Dependent Variable\n",
        "\n",
        "    # Variance Thresholding\n",
        "    if sel == 'var':\n",
        "        threshold = addi['threshold'][0]\n",
        "        select = VarianceThreshold(threshold=threshold) # Selecting the threshold\n",
        "        select.fit(X)  # Fitting the variable\n",
        "        features = X.columns[select.get_support(indices=True)].tolist()\n",
        "\n",
        "    # Permutation Importance\n",
        "    elif sel == 'imp':\n",
        "        threshold = addi['threshold'][1]\n",
        "        model_imp = addi['imp_func']()\n",
        "        model_imp.fit(X, y)\n",
        "        results_imp = permutation_importance(model_imp, X, y, n_repeats=30)\n",
        "        features = X.columns[results_imp.importances_mean >= threshold].tolist()\n",
        "\n",
        "    # Pearson Correlation\n",
        "    elif sel == 'cor':\n",
        "        threshold = addi['threshold'][2]\n",
        "        corr_mat = df[num_cols + [target]].corr().abs()\n",
        "        corr_target = corr_mat[target].drop(target)\n",
        "        features = corr_target[corr_target > threshold].index.tolist()\n",
        "\n",
        "    # LASSO Regularization\n",
        "    elif sel == 'lasso':\n",
        "        # cv=5 number of folds in cross-validation\n",
        "        model_lasso = LassoCV(cv=5, random_state=42) # random_state= 42 for reproducbility\n",
        "        model_lasso.fit(X, y)\n",
        "        features = X.columns[model_lasso.coef_ != 0].tolist()\n",
        "    else:\n",
        "      return 'The Feature selection technique specified is incorrect, please use the correct feature selection technique'\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "Kg8wWpq6I3Dc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_perf(*args):\n",
        "    \"\"\"\n",
        "    Intro: The function is used to build, train, test and evaluate a logistic regression model and its perdictions for target variable of '5yrs' by using each of the previously implemented feature selection techniques\n",
        "\n",
        "    Parameters used:\n",
        "    - df (DataFrame): The dataset used to build the model.\n",
        "    - target (str): The dependent variable/target column to be predicted.\n",
        "    - addi (dict): A dictionary storing technique-specific additional variables.\n",
        "\n",
        "    Functionality:\n",
        "    - dropna() - drops NaN values, subsequently we can also treat these missing values using Mean or Median\n",
        "    - for method in methods- Iterates over different feature selection methods ('var', 'imp', 'cor', 'lasso'), For each method, it uses the Fextractor function to select relevant features.\n",
        "    - train_test_split - splits the data into training and testing sets using train_test_split from SKlearn\n",
        "    - fit() - Fits the LogisticRegression model(Classification model) on the training data\n",
        "    - y_pred, y_pred_proba - Makes predictions on the test data and evaluates the performance using metrics like MSE, accuracy, and AUC.\n",
        "    - results_plot - Stores the results in a list\n",
        "    - df_results - creates a DataFrame with the results stored\n",
        "    - plt.plot(fpr, tpr, label=f\"{method} (AUC = {roc_auc:.3f})\") - Plots the ROC (Receiver Operating Characteristic) curves for each model.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: It returns a DataFrame containing the performance metrics such as Accuracy(ACC% Accuracy Percentage), MSE(Mean Squared Error) and AUC(Area Under the Curve)\n",
        "    - It also plots the ROC curve with the different feature selection techniques\n",
        "\n",
        "    Note:\n",
        "      The function does not employ feature scaling (e.g., StandardScaler) since the library was not imported in the test case setup. The absence of scaling is very evident which, affects model convergence when running the test case by potential convergence warnings when the model and hits the iteration limit before fully converging.\n",
        "      I have used 'max_iter = 1000' to see if it helps but no change to the convergence warning.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack the arugments and assign to args\n",
        "    df, target, addi = args\n",
        "\n",
        "    # Drops the NAN values in the dataframe\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Set the size of ROC curves plot\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # results list to be returned\n",
        "    results_plot = []\n",
        "\n",
        "    # Methods from Fextractor for feature selection\n",
        "    methods = ['var', 'imp', 'cor', 'lasso']\n",
        "\n",
        "    for method in methods:\n",
        "        #  Intergrate the Fextractor function to select features based on the method\n",
        "        selected_features = Fextractor(df, target, method, addi)\n",
        "        if not selected_features:\n",
        "            # If no features were returned\n",
        "            print(f\"No features selected using method: {method}\")\n",
        "            continue\n",
        "\n",
        "        X = df[selected_features]\n",
        "        y = df[target]\n",
        "\n",
        "        # Splitting the dataset into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Fitting Logistic Regression\n",
        "        model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions and Evaluations\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        mse = np.mean((y_pred - y_test.values) ** 2)  # Calculate MSE manually\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)  # ROC curve\n",
        "        roc_auc = auc(fpr, tpr)  # AUC score\n",
        "\n",
        "        # Storing results\n",
        "        results_plot.append({'Model': method, 'ACC%': acc * 100, 'MSE': mse, 'AUC': roc_auc})\n",
        "\n",
        "        # Create a DataFrame with results and print it\n",
        "        df_results = pd.DataFrame(results_plot)\n",
        "\n",
        "        # Plot ROC curve for this model\n",
        "        plt.plot(fpr, tpr, label=f\"{method} (AUC = {roc_auc:.3f})\")\n",
        "    print(df_results, '\\n')\n",
        "\n",
        "    # Finalize and show the ROC plot\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Different Feature Selection Methods')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "B1D9oF6vzT6Q"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}